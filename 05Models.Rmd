---
title: "Model Basics"
output: 
  xaringan::moon_reader:
    nature:
      ratio: '16:9'
---

<style>
div.footnotes {
position: absolute;
bottom: 0;
margin-bottom: 10px;
width: 80%;
font-size: 0.6em;
}

</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, fig.width = 8, fig.height= 4.5)
library(tidyverse)
library(knitr)
```

### Tools for successful model building
Model generation and model confirmation are important pieces of the overall modeling process to ensure that your generated model can be applied generally.

Tools to ensure this are:
- Dividing data into Training/Validation/Test subsets
- Cross-validation

These tools are important in reducing the chance of "overfitting" your model to a specific data set.

---
### Introduction
The modeling process includes 2 parts:
1. Decide on a **family of models**
2. Generate your **fitted model**

A fitted model is just the best model from a family of models.

George Box, a famous statistician, once said: "All models are wrong, but some are useful."


---
### Prerequisites
In order for modeling to work well with pipes, we're going to load the `modelr` package.

```{r echo = TRUE}
library(tidyverse)

library(modelr)
options(na.action = na.warn)
```

Notice we are also setting an option to warn us whenever observations with missing values are going to be dropped from our model.


---
### Intuition of a Simple Linear Model
Let's take a look at the simulated dataset `sim1`, included with the `modelr` package. It contains two continuous variables, `x` and `y`. Let's plot them to see how they’re related:

```{r echo=TRUE}
ggplot(sim1, aes(x, y)) + 
  geom_point()

```


---
### Intuition of a Simple Linear Model (cont.)
Use a simple linear model to describe the relationship: `y = m * x + b`. Let's generate a bunch of `m` and `b` combinations and see which models look best.

```{r echo=TRUE, fig.height=4}
models <- tibble(
  b = runif(250, -20, 40),
  m = runif(250, -5, 5)
)

ggplot(sim1, aes(x, y)) + 
  geom_abline(aes(intercept = b, slope = m), data = models, alpha = 1/4) +
  geom_point() 
```

A lot of these models are bad.  How can we determine which are best?



---
### Residual Sum of Squares
One way to determine the best model is to compute the residual sum of squares (RSS) for each model and find the model with the lowest value.

```{r, echo = FALSE}
dist1 <- sim1 %>% 
  mutate(
    dodge = rep(c(-1, 0, 1) / 20, 10),
    x1 = x + dodge,
    pred = 7 + x1 * 1.5
  )
ggplot(dist1, aes(x1, y)) + 
  geom_abline(intercept = 7, slope = 1.5, colour = "grey40") +
  geom_point(colour = "grey40") +
  geom_linerange(aes(ymin = y, ymax = pred), colour = "#3366FF") 
```

You may have heard the term "Ordinary Least Squares" or "OLS Regression". This refers to the process of finding the best model by minimizing the RSS discussed above.


---
###  Residual Sum of Squares (cont.)
```{r}
model1 <- function(a, data) {
  a[1] + data$x * a[2]
}

measure_distance <- function(mod, data) {
  diff <- data$y - model1(mod, data)
  sqrt(mean(diff ^ 2))
}

sim1_dist <- function(a1, a2) {
  measure_distance(c(a1, a2), sim1)
}

models <- models %>% 
  mutate(dist = purrr::map2_dbl(b, m, sim1_dist))
```

Next, let’s overlay the 10 best models on to the data. 

```{r echo=TRUE}
ggplot(sim1, aes(x, y)) + 
  geom_point(size = 2, colour = "grey30") + 
  geom_abline(
    aes(intercept = b, slope = m, colour = -dist), 
    data = filter(models, rank(dist) <= 10)
  )
```


---
###  Residual Sum of Squares (cont.)
We can also think about these models as observations, and visualising with a scatterplot of b vs m:

```{r echo=TRUE}
ggplot(models, aes(b, m)) +
  geom_point(data = filter(models, rank(dist) <= 10), size = 4, colour = "red") +
  geom_point(aes(colour = -dist))
```



---
###  Residual Sum of Squares (cont.)
Let's zoom in a bit around the combination of best models and do one more iteration to find even better models:

```{r}
grid <- expand.grid(
  b = seq(-5, 20, length = 25),
  m = seq(1, 3, length = 25)
  ) %>% 
  mutate(dist = purrr::map2_dbl(b, m, sim1_dist))

grid %>% 
  ggplot(aes(b, m)) +
  geom_point(data = filter(grid, rank(dist) <= 10), size = 4, colour = "red") +
  geom_point(aes(colour = -dist)) 
```


---
###  Residual Sum of Squares (cont.)
When you overlay the best 10 models back on the original data, they all look pretty good:

```{r echo=TRUE}
ggplot(sim1, aes(x, y)) + 
  geom_point(size = 2, colour = "grey30") + 
  geom_abline(
    aes(intercept = b, slope = m, colour = -dist), 
    data = filter(grid, rank(dist) <= 10)
  )
```



---
###  Best Linear Model
Well, it turns out that there is a mathematical formula that is guaranteed to always return the linear equation with the minimum residual sum of squares. This formula is implemented in the `lm()` function.

```{r echo=TRUE}
sim1_mod <- lm(y ~ x, data = sim1)

grid <- sim1 %>% 
  data_grid(x) %>% 
  add_predictions(sim1_mod)

ggplot(sim1, aes(x)) +
  geom_point(aes(y = y)) +
  geom_line(aes(y = pred), data = grid, colour = "red", size = 1)

```


---
###  Best Linear Model (cont.)
It's always a good practice to check the distribution of your residuals to ensure that you have captured the relationship well with your model.

```{r echo=TRUE}
sim1 <- sim1 %>% add_residuals(sim1_mod)

ggplot(sim1, aes(x, resid)) + 
  geom_ref_line(h = 0) +
  geom_point() 
```

The residuals appear to be randomly scattered across the values of x. This is what we expect in a good model.


---
### Linear Model with Categorical Variables
Generating a function from a formula is straight forward when the predictor is continuous, but things get a bit more complicated when the predictor is categorical.

```{r echo=TRUE}
ggplot(sim2) + 
  geom_point(aes(x, y))
```



---
### Linear Model with Categorical Variables (cont.)
Effectively, a model with a categorical x will predict the mean value for each category.

```{r echo=TRUE}
mod2 <- lm(y ~ x, data = sim2)
grid <- sim2 %>% data_grid(x) %>% add_predictions(mod2)

ggplot(sim2, aes(x)) + 
  geom_point(aes(y = y)) +
  geom_point(data = grid, aes(y = pred), colour = "red", size = 4)
```



---
### Linear Model with Interactions
What happens when you combine a continuous and a categorical variable? `sim3` contains a categorical predictor and a continuous predictor. We can visualise it with a simple plot:

```{r echo=TRUE}
ggplot(sim3, aes(x1, y)) + 
  geom_point(aes(colour = x2))
```



---
### Linear Model with Interactions (cont.)
There are two possible models you could fit to this data:

```{r echo=TRUE}
mod1 <- lm(y ~ x1 + x2, data = sim3)
mod2 <- lm(y ~ x1 * x2, data = sim3)
```

Let's calculate the predicted values for this model in order to visualize:

```{r echo=TRUE}
grid <- sim3 %>% 
  data_grid(x1, x2) %>% 
  gather_predictions(mod1, mod2)
```


---
### Linear Model with Interactions (cont.)
We can visualise the results for both models on one plot using facetting:

```{r}
ggplot(sim3, aes(x1, y, colour = x2)) + 
  geom_point() + 
  geom_line(data = grid, aes(y = pred)) + 
  facet_wrap(~ model)
```

Note that the model that uses `+` has the same slope for each line, but different intercepts. The model that uses `*` has a different slope and intercept for each line.



---
### Linear Model with Interactions (cont.)
Which model is better?  Well, let's take a look at the residuals:

```{r fig.width=11, fig.height=6}
sim3 <- sim3 %>% 
  gather_residuals(mod1, mod2)

ggplot(sim3, aes(x1, resid, colour = x2)) + 
  geom_point() + 
  facet_grid(model ~ x2)
```


---
### Other Model Families
- Generalised linear models, e.g. `stats::glm()`
- Generalised additive models, e.g. `mgcv::gam()`
- Penalised linear models, e.g. `glmnet::glmnet()`
- Robust linear models, e.g. `MASS:rlm()`
- Trees, e.g. `rpart::rpart()`, `randomForest::randomForest()`, `xgboost::xgboost()`


---
### Case Study: The Titanic
